database: ./database.db3

# the folder from which to start the backup
root: /path/to/root

# list of files to ignore. Syntax: https://docs.rs/globset/latest/globset/#syntax
ignore:
  - "**/Thumbs.db"
  - "**/.DS_Store"

# total memory consumption is given by `chunks.size * (workers + queue_size)`
# i.e. 90MB chunks * 8 in progress + 8 in the queue = ~1.5GB RAM

# max amount of worker threads. Valid values: >= 1
workers: 8
# max job queue length. With large enough chunks, keeping this value equal
# to the worker's count maximizes the throughput and minimizes the total RAM
# consumption. Valid values: >= 0
queue_size: 8

# optional, but if set, a cache folder containing decrypted chunks being accessed through the FUSE filesystem
#cache: ...

chunks:
  # default 100MB
  size: 500MB

# aggregate small files? files smaller than min_size will be aggregated in blocks of max size (size must be <=
# chunks.size)
aggregate:
  # if not set or set to 0, no aggregation is made
  min_size: 100MB
  # if not set, chunks.size is used
  size: 500MB

pgp:
  # a public key to push, a keypair for pull/fuse
  key: ...
#  # for password-protected keypair
#  passphrase: ...

store:
#   possible values:
#   - log          just display logs about what is happening
#   - s3           uploads chunks to AWS S3
#   - s3-official  uploads chunks to AWS S3, using official S3 client (for credentials and region configuration,see
#                  https://docs.aws.amazon.com/sdk-for-rust/latest/dg/getting-started.html)
#   - local        moves data to a local folder
  type: log
#  s3-official:
#    bucket: ...
#    # if > 5MiB (5242880 Bytes), uploads the chunks in parts of multipart_part_size.
#    multipart_part_size: 10MB
#  s3:
#    access_key: ...
#    secret_key: ...
#    region: ...
#    bucket: ...
#  local:
#    # the folder will be created if it does not exist
#    path: ...