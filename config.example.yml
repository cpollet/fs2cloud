database: ./database.db3

# total memory consumption is given by `chunks.size * (workers + queue_size)`
# i.e. 90MB chunks * 8 in progress + 8 in the queue = ~1.5GB RAM

# max amount of worker threads. Valid values: >= 1
workers: 8
# max job queue length. With large enough chunks, keeping this value equal
# to the worker's count maximizes the throughput and minimizes the total RAM
# consumption. Valid values: >= 0
queue_size: 8

chunks:
  size: 90MB

pgp:
  # a public key to push, a keypair for pull/fuse
  key: ...
#  # for password-protected keypair
#  passphrase: ...

store:
#   possible values:
#   - log          just display logs about what is happening
#   - s3           uploads chunks to AWS S3
#   - s3-official  uploads chunks to AWS S3, using official S3 client (for credentials and region configuration,see
#                  https://docs.aws.amazon.com/sdk-for-rust/latest/dg/getting-started.html)
#   - local        moves data to a local folder
  type: log
#  s3-official:
#    bucket: ...
#    # if > 5MiB (5242880 Bytes), uploads the chunks in parts of multipart_part_size.
#    multipart_part_size: ...
#  s3:
#    access_key: ...
#    secret_key: ...
#    region: ...
#    bucket: ...
#  local:
#    # the folder will be created if it does not exist
#    path: ...